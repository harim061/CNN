{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIOALkBQw6d0DsfQEcBqvq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harim061/CNN/blob/main/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C3FOpYoVSHfA"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "\tdef __init__(self, block):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.block = block\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.block(x) + x #f(x) + x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- self.block(x)는 입력 x가 block을 통과한 후의 출력값 = F(x)\n",
        "- 원래의 입력 x와 F(x)를 더하여 최종 출력값을 생성 -> H(x) = F(x) + x"
      ],
      "metadata": {
        "id": "N_C7btbSeIzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN에 넣기"
      ],
      "metadata": {
        "id": "S1ppZFPTdpJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv6(nn.Module):\n",
        "  def __init__(self, n_class=10):\n",
        "    super().__init__()\n",
        "    self.name = 'conv6'\n",
        "    self.model = nn.Sequential(\n",
        "\n",
        "        # nn.Conv2d(입력채널수, 출력채널수(32개 필터), 커널(필터)크기, 스트라이드, 패딩)\n",
        "        nn.Conv2d(3, 32, 3, 1, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Conv2d(32, 32, 3, 1, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Conv2d(32, 32, 3, 1, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Conv2d(32, 32, 3, 1, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Conv2d(32, 32, 3, 1, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Conv2d(32, 32, 3, 1, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(32*32*32, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 10)\n",
        "\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "NKwk7wQXdq0r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 몇몇 레이어를 residual block으로 묶기\n",
        "class Conv6Res(nn.Module):\n",
        "  def __init__(self, n_class=10):\n",
        "    super().__init__()\n",
        "    self.name = 'conv6res'\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, 3, 1, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        ResBlock(\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(32, 32, 3, 1, 1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU(),\n",
        "\n",
        "                nn.Conv2d(32, 32, 3, 1, 1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        ),\n",
        "\n",
        "        ResBlock(\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(32, 32, 3, 1, 1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU(),\n",
        "\n",
        "                nn.Conv2d(32, 32, 3, 1, 1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU(),\n",
        "\n",
        "                nn.Conv2d(32, 32, 3, 1, 1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "        ),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(32*32*32, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 10)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "ygXWSF0bfEfZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR10 데이터셋에 대해 트레이닝하여 비교"
      ],
      "metadata": {
        "id": "gkSCoN-3fRzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "# Downloading the CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "cifar_tr = datasets.CIFAR10(root=os.getcwd(), train=True, download=True, transform=transform)\n",
        "cifar_test = datasets.CIFAR10(root=os.getcwd(), train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# Split training data into train set and validation set\n",
        "def split_train_valid(dataset, valid_ratio=0.1):\n",
        "  data_size = len(dataset)\n",
        "  indices = list(range(data_size))\n",
        "  np.random.seed(1)\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  split_point = int(np.floor(valid_ratio*data_size))\n",
        "  val_index, train_index = indices[:split_point-1], indices[split_point:]\n",
        "\n",
        "  train = torch.utils.data.Subset(dataset, train_index)\n",
        "  valid = torch.utils.data.Subset(dataset, val_index)\n",
        "\n",
        "  return train, valid\n",
        "\n",
        "cifar_train, cifar_valid = split_train_valid(dataset=cifar_tr)\n",
        "\n",
        "# Make DataLoaders for train/validation/test sets\n",
        "cifar_loaders = [DataLoader(dataset=d, batch_size=128, shuffle=True, drop_last=True) for d in [cifar_train, cifar_valid, cifar_test]]\n",
        "\n",
        "# Define model to train\n",
        "model = Conv6()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "# 손실 함수 정의\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Use GPU if available (CPU if not)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train the model\n",
        "print(\"===== Train Start =====\")\n",
        "num_epochs = 40\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"valid_loss\":[], \"valid_acc\":[]} # record of loss and accuracy in each epoch for plotting later\n",
        "for epoch in range(num_epochs):\n",
        "\ttrain_loss, train_acc = 0, 0\n",
        "\tmodel.train()\n",
        "\tfor (x, y) in cifar_loaders[0]: #cifar_loaders[0] is train set DataLoader\n",
        "\t\tx = x.to(device)\n",
        "\t\ty = y.to(device)\n",
        "\n",
        "\t\ty_hat = model(x)\n",
        "\t\tloss_val = loss_fn(y_hat, y)\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tloss_val.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\ttrain_loss += loss_val.to(\"cpu\").item()\n",
        "\t\ttrain_acc += (y_hat.argmax(1)==y).type(torch.float).to('cpu').mean().item()\n",
        "\n",
        "\ttrain_loss /= len(cifar_loaders[0]) #len(DataLoader) is batch size (ie, 128)\n",
        "\ttrain_acc /= len(cifar_loaders[0])\n",
        "\thistory[\"train_loss\"].append(train_loss)\n",
        "\thistory[\"train_acc\"].append(train_acc)\n",
        "\n",
        "\t# Evaluate model on validation set\n",
        "\tvalid_loss, valid_acc = 0, 0\n",
        "\tmodel.eval()\n",
        "\twith torch.no_grad():\n",
        "\t\tfor (x, y) in cifar_loaders[1]: # Validation set DataLoader\n",
        "\t\t\tx = x.to(device)\n",
        "\t\t\ty = y.to(device)\n",
        "\n",
        "\t\t\ty_hat = model(x)\n",
        "\t\t\tloss_val = loss_fn(y_hat, y)\n",
        "\n",
        "\t\t\tvalid_loss += loss_val.to('cpu').item()\n",
        "\t\t\tvalid_acc += (y_hat.argmax(1)==y).type(torch.float).to('cpu').mean().item()\n",
        "\n",
        "\tvalid_loss /= len(cifar_loaders[1]) #len(DataLoader) is batch size\n",
        "\tvalid_acc /= len(cifar_loaders[1])\n",
        "\thistory[\"valid_loss\"].append(valid_loss)\n",
        "\thistory[\"valid_acc\"].append(valid_acc)\n",
        "\n",
        "\tif epoch % 10 == 0:\n",
        "\t  print(f\"Epoch: {epoch}, train loss: {train_loss:>6f}, train acc: {train_acc:>3f}, valid loss: {valid_loss:>6f}, valid acc: {valid_acc:>3f}\")\n",
        "\n",
        "# Test the model on the test set\n",
        "print(\"===== Test Start =====\")\n",
        "test_loss, test_acc = 0, 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for (x, y) in cifar_loaders[2]:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        y_hat = model(x)\n",
        "        loss_val = loss_fn(y_hat, y)\n",
        "\n",
        "        test_loss += loss_val.to('cpu').item()\n",
        "        test_acc += (y_hat.argmax(1)==y).type(torch.float).to('cpu').mean().item()\n",
        "\n",
        "test_loss /= len(cifar_loaders[2])\n",
        "test_acc /= len(cifar_loaders[2])\n",
        "print(f\"Test loss: {epoch_loss:>6f}, Test acc: {epoch_acc:>6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "-bGUv0onfSLe",
        "outputId": "882808d4-75b6-4842-83c5-a9b8ccd56010"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "===== Train Start =====\n",
            "Epoch: 0, train loss: 1.515480, train acc: 0.473090, valid loss: 1.187902, valid acc: 0.584135\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-7d5541b0bc04>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-77f483da50a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "# Downloading the CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "cifar_tr = datasets.CIFAR10(root=os.getcwd(), train=True, download=True, transform=transform)\n",
        "cifar_test = datasets.CIFAR10(root=os.getcwd(), train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# Split training data into train set and validation set\n",
        "def split_train_valid(dataset, valid_ratio=0.1):\n",
        "  data_size = len(dataset)\n",
        "  indices = list(range(data_size))\n",
        "  np.random.seed(1)\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  split_point = int(np.floor(valid_ratio*data_size))\n",
        "  val_index, train_index = indices[:split_point-1], indices[split_point:]\n",
        "\n",
        "  train = torch.utils.data.Subset(dataset, train_index)\n",
        "  valid = torch.utils.data.Subset(dataset, val_index)\n",
        "\n",
        "  return train, valid\n",
        "\n",
        "cifar_train, cifar_valid = split_train_valid(dataset=cifar_tr)\n",
        "\n",
        "# Make DataLoaders for train/validation/test sets\n",
        "cifar_loaders = [DataLoader(dataset=d, batch_size=128, shuffle=True, drop_last=True) for d in [cifar_train, cifar_valid, cifar_test]]\n",
        "\n",
        "# Define model to train\n",
        "model = Conv6Res()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "# 손실 함수 정의\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Use GPU if available (CPU if not)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train the model\n",
        "print(\"===== Train Start =====\")\n",
        "num_epochs = 40\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"valid_loss\":[], \"valid_acc\":[]} # record of loss and accuracy in each epoch for plotting later\n",
        "for epoch in range(num_epochs):\n",
        "\ttrain_loss, train_acc = 0, 0\n",
        "\tmodel.train()\n",
        "\tfor (x, y) in cifar_loaders[0]: #cifar_loaders[0] is train set DataLoader\n",
        "\t\tx = x.to(device)\n",
        "\t\ty = y.to(device)\n",
        "\n",
        "\t\ty_hat = model(x)\n",
        "\t\tloss_val = loss_fn(y_hat, y)\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tloss_val.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\ttrain_loss += loss_val.to(\"cpu\").item()\n",
        "\t\ttrain_acc += (y_hat.argmax(1)==y).type(torch.float).to('cpu').mean().item()\n",
        "\n",
        "\ttrain_loss /= len(cifar_loaders[0]) #len(DataLoader) is batch size (ie, 128)\n",
        "\ttrain_acc /= len(cifar_loaders[0])\n",
        "\thistory[\"train_loss\"].append(train_loss)\n",
        "\thistory[\"train_acc\"].append(train_acc)\n",
        "\n",
        "\t# Evaluate model on validation set\n",
        "\tvalid_loss, valid_acc = 0, 0\n",
        "\tmodel.eval()\n",
        "\twith torch.no_grad():\n",
        "\t\tfor (x, y) in cifar_loaders[1]: # Validation set DataLoader\n",
        "\t\t\tx = x.to(device)\n",
        "\t\t\ty = y.to(device)\n",
        "\n",
        "\t\t\ty_hat = model(x)\n",
        "\t\t\tloss_val = loss_fn(y_hat, y)\n",
        "\n",
        "\t\t\tvalid_loss += loss_val.to('cpu').item()\n",
        "\t\t\tvalid_acc += (y_hat.argmax(1)==y).type(torch.float).to('cpu').mean().item()\n",
        "\n",
        "\tvalid_loss /= len(cifar_loaders[1]) #len(DataLoader) is batch size\n",
        "\tvalid_acc /= len(cifar_loaders[1])\n",
        "\thistory[\"valid_loss\"].append(valid_loss)\n",
        "\thistory[\"valid_acc\"].append(valid_acc)\n",
        "\n",
        "\tif epoch % 10 == 0:\n",
        "\t  print(f\"Epoch: {epoch}, train loss: {train_loss:>6f}, train acc: {train_acc:>3f}, valid loss: {valid_loss:>6f}, valid acc: {valid_acc:>3f}\")\n",
        "\n",
        "# Test the model on the test set\n",
        "print(\"===== Test Start =====\")\n",
        "test_loss, test_acc = 0, 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for (x, y) in cifar_loaders[2]:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        y_hat = model(x)\n",
        "        loss_val = loss_fn(y_hat, y)\n",
        "\n",
        "        test_loss += loss_val.to('cpu').item()\n",
        "        test_acc += (y_hat.argmax(1)==y).type(torch.float).to('cpu').mean().item()\n",
        "\n",
        "test_loss /= len(cifar_loaders[2])\n",
        "test_acc /= len(cifar_loaders[2])\n",
        "print(f\"Test loss: {epoch_loss:>6f}, Test acc: {epoch_acc:>6f}\")"
      ],
      "metadata": {
        "id": "S0hnxI_liJb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 두 모델의 accuracy는 별 차이 없지만, residual block은 뉴럴 네트워크의 레이어 갯수가 점점 많아지면서 오히려 performance가 저하 되는 'degradation'현상을 막아주는 것이 주 역할이므로 레이어가 더 많은 deeper 뉴럴 네트워크들에서 residual block의 효과를 봄"
      ],
      "metadata": {
        "id": "rXnaasaph6QG"
      }
    }
  ]
}